Example 1

Write a Python program that implements a distributed task queue system using a combination of RabbitMQ for message brokering and workers for task execution. The program should:

1. Use RabbitMQ to queue tasks and ensure tasks are durable, so they are not lost if the broker restarts.
2. Include a producer module that accepts tasks from users via an API (e.g., Flask).
3. Include a worker module that pulls tasks from the queue, processes them, and stores results in a MongoDB database.
4. Handle multiple worker instances to distribute the task load efficiently.
5. Provide a monitoring dashboard using Flask-SocketIO to display the status of tasks in real time (pending, processing, completed).
6. Implement logging for critical events (e.g., task received, task completed, errors) and allow configurable logging levels.
7. Include exception handling to manage potential failures, such as RabbitMQ connection errors or task execution issues.

Ensure the code follows best practices for modularity and reusability, and add comments explaining the logic of key components.

Example 2


Write a Java program that implements a distributed task processing system using Apache Kafka for message brokering. The program should:

1. Use Kafka to queue tasks with appropriate configurations, such as partitions and replication, to ensure fault tolerance and scalability.
2. Include a producer module that accepts tasks from users via a REST API (e.g., using Spring Boot) and sends these tasks to a Kafka topic.
3. Include a consumer module that listens to the Kafka topic, processes the tasks, and stores the results in a MySQL database.
4. Support multiple consumer instances to enable task distribution and parallel processing.
5. Provide a monitoring dashboard using Spring Boot Actuator or a lightweight frontend to display task status (pending, processing, completed) in real time by integrating with Kafka Streams or querying the database.
6. Implement detailed logging with SLF4J for critical events (e.g., task submission, processing completion, errors) and allow for configurable logging levels.
7. Handle exceptions gracefully, including Kafka connection errors, message deserialization failures, and database write issues.
8. Ensure modularity, with clear separation between producer, consumer, and utility classes, and add comments to explain the logic of key components.

Make sure the code adheres to Java best practices, follows clean coding principles, and includes unit tests for core functionalities.

Prompt for a Complex Code Generation

Write a Java program that implements a distributed task processing system using Apache Kafka for message brokering, along with a front-end interface for monitoring and management. The program should:

1. **Backend Implementation**:
   - Use Kafka to queue tasks with proper configurations for fault tolerance (e.g., partitions, replication) and ensure efficient task distribution.
   - Include a producer module using Spring Boot to accept tasks via a REST API and send them to a Kafka topic.
   - Include a consumer module that listens to Kafka topics, processes tasks, and stores the results in a MySQL database.
   - Enable multiple consumer instances to support parallel task processing.
   - Implement Kafka Streams for aggregating task statuses (e.g., pending, processing, completed) and maintaining an up-to-date status log.

2. **Frontend Implementation**:
   - Build a web-based interface using React (or Angular) for task monitoring and management:
     - **Dashboard**: Display the list of tasks with their statuses (pending, processing, completed), timestamps, and any associated metadata.
     - **Management Features**: Provide controls for pausing, resuming, or retrying failed tasks.
     - **Real-time Updates**: Integrate WebSockets or long polling to fetch task statuses in real time from the backend.
     - **Filter and Search**: Allow users to search and filter tasks based on attributes like status, timestamps, or task type.

3. **Integration**:
   - Use Spring Boot to serve the backend APIs and connect the front-end application to fetch task data and trigger management actions.
   - Leverage Kafkaâ€™s built-in tools and APIs for administrative actions, such as rebalancing partitions or querying offsets, and expose these as backend services.

4. **Exception Handling**:
   - Implement robust error handling for backend processes, such as Kafka connection failures, message deserialization issues, or database write errors.
   - Provide meaningful error messages and status updates on the frontend when operations fail.

5. **Logging and Monitoring**:
   - Use SLF4J for detailed logging of critical events, such as task creation, processing, and completion, with configurable log levels.
   - Integrate Spring Boot Actuator and Prometheus/Grafana for backend monitoring and metrics visualization.

6. **Testing**:
   - Include unit tests for producer, consumer, and API endpoints using JUnit.
   - Add integration tests to verify the interaction between Kafka, the database, and the front-end application.

7. **Documentation**:
   - Provide comprehensive comments and a README file explaining how to set up and run the system, including backend services, Kafka configuration, and the front-end application.

Why This Works
The prompt now specifies both backend and frontend requirements, creating a full-stack system.
It provides detailed guidance for Kafka's messaging layer while outlining specific features for user interactivity and real-time monitoring.
Copilot can generate modular components incrementally based on the defined structure.



*******


This entire context is based on predictive analysis and for this you use Python technology and To make this framework more dynamic and AI-driven, the following updates can be introduced to the existing system to predict user actions, manage caching intelligently, and dynamically adapt based on usage patterns:

1. **Backend**:
   - This is an existing table that captures user actions from a web application
   - These user actions are stored in a database table called event_logs
   - these user actions are associated to the employee id and the time that user performed action like build, explore, knowledgebase, tooling, manage etcs
   
2. **Predictive analysis expectation**:
     - Primarily two scenarions:
     	- **Scenario 1**: In this case when user login to the application for the first time and has never ever performed any action on the application, I want to show a message on about how to use applcation, tutorials, videos to use best out of the application
     	- **Scenario 2**: User have been to the application and user has done many things on the application and in this case we want to show user on mostly action events by the user and show to the user and ask user if he or she would like to do the same again. examples would be user did Oracle patching 5 files in last 10 days, Linux build 4 times and all others are like 1 or two times then we should Oracle or linux activitiy that user performed
     	
3. **Development**:
   - Develop python based framework and APIs that would be called from the a web application from where user actions are captured and return a json structure to show outcome of above two scenarios
   - API needs to be performance driven and should be 1-2 seconds response time and to achieve that if python framework needs to create a cache of the user action then do that
   - API is going to be a readonly operations and do not need to have any sort of authentication
   - API would expect the user information to which then need to map to user action and do all predictive analysis work

3. **Integration**:
   - Web application that captures user action would then need to make a call to that API and show result.
   
4. **Documentation**:
   - Provide comprehensive comments and a README file explaining how to set up and run the system, including backend services, Kafka configuration, and the front-end application.


